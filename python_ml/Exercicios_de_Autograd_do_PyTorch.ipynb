{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5_ZRWRfCZtI"
   },
   "source": [
    "# Exercício PyTorch: Gradientes e Grafo Computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMoyGt5gXMgK"
   },
   "source": [
    "## Coloque seu nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBHbXcibXPRe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é: Renato César Alves de Oliveira\n"
     ]
    }
   ],
   "source": [
    "print('Meu nome é: Renato César Alves de Oliveira')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIlQdKAuCZtR"
   },
   "source": [
    "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de\n",
    "calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada\n",
    "pelo tensor através do cálculo automático do gradiente pela construção dinâmica do grafo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF_-dJ2nCZtT"
   },
   "source": [
    "## Grafo computacional\n",
    "\n",
    "Seja um exemplo simples de uma função de Perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
    "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
    "que pode ser reescrita como:\n",
    "$$ \\hat{y_i} = x_i w $$\n",
    "$$ e_i = \\hat{y_i} - y_i $$\n",
    "$$ e2_i = e_i^2 $$\n",
    "$$ J = \\sum_i e2_i $$\n",
    "\n",
    "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
    "regra da cadeia podemos escrever:\n",
    "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jboejVQMCZtU"
   },
   "source": [
    "```\n",
    "    y_pred = x * w\n",
    "    e = y_pred - y\n",
    "    e2 = e**2\n",
    "    J = e2.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7JmU6qhc2Y2"
   },
   "source": [
    "As quatro expressões acima, para o cálculo do J pode ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KeeEBKl4CZtV"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yZun7wrCZtX"
   },
   "source": [
    "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T00:23:00.431853Z",
     "start_time": "2019-12-11T00:23:00.414813Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HlT2d-4fCZtZ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T00:23:00.863228Z",
     "start_time": "2019-12-11T00:23:00.844457Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xX0QwUduCZtf",
    "outputId": "44dd7344-6777-4a9f-efb0-b4719cddb1f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsqzALS4CZtl"
   },
   "source": [
    "## Tensor com atributo .requires_grad=*True*\n",
    "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:22.117010Z",
     "start_time": "2019-09-29T03:07:22.041861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "foaAb94aCZtm",
    "outputId": "6b4ffaab-907b-45c0-8e52-b87de2884c71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(0, 8, 2).float()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:28.610934Z",
     "start_time": "2019-09-29T03:07:28.598223Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "no6SdSyICZtr",
    "outputId": "2ea356e4-ed8e-4647-d327-2e08a4354099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 4).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:31.523762Z",
     "start_time": "2019-09-29T03:07:31.497683Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eL_i1mwGCZtw",
    "outputId": "60e7a2b3-f367-4da0-96d9-8988e5929e48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjEl-0l7CZt0"
   },
   "source": [
    "## Cálculo automático do gradiente da função perda J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pUh-SCnCZt1"
   },
   "source": [
    "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
    "\n",
    "Queremos calcular a derivada de $J$ em relação a $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMwwVtJ1CZt2"
   },
   "source": [
    "### Montagem do grafo computacional - forward\n",
    "\n",
    "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:36.290122Z",
     "start_time": "2019-09-29T03:07:36.273229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "zp2aK4YhCZt3",
    "outputId": "fd254528-941f-4edf-8d7a-854b0345f9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
      "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
      "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
      "J = tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# predict (forward)\n",
    "y_pred = x * w; print('y_pred =', y_pred)\n",
    "\n",
    "# cálculo da perda J: loss\n",
    "e = y_pred - y; print('e =',e)\n",
    "e2 = e.pow(2) ; print('e2 =', e2)\n",
    "J = e2.sum()  ; print('J =', J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC96wB7PCZt8"
   },
   "source": [
    "## Auto grad - processa o grafo computacional backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T15:55:45.308858",
     "start_time": "2017-10-04T15:55:45.304654"
    },
    "colab_type": "text",
    "id": "kKbf4D0CCZt-"
   },
   "source": [
    "O `backward()` varre o grafo computacional a partir da variável a ele associada e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
    "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. Entretanto, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-CjLPu6clVo"
   },
   "outputs": [],
   "source": [
    "e2.retain_grad()\n",
    "e.retain_grad()\n",
    "y_pred.retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtsZS2Bicof-"
   },
   "source": [
    "E agora calculamos os gradientes com o `backward()`.\n",
    "\n",
    "w.grad é o gradiente de J em relação a w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:40.267334Z",
     "start_time": "2019-09-29T03:07:40.247422Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z1lnkb0GCZt_",
    "outputId": "1cf5d1b6-e2ae-4e48-c40e-29454be5adc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-28.])\n"
     ]
    }
   ],
   "source": [
    "if w.grad: w.grad.zero_()\n",
    "J.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1xYDPR_uOcZ"
   },
   "source": [
    "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Enuk2tf0sDyO",
    "outputId": "9975ac15-dfe7-4c8e-df59-f691455cd478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([ 0., -2., -4., -6.])\n",
      "tensor([ 0., -2., -4., -6.])\n"
     ]
    }
   ],
   "source": [
    "print(e2.grad)\n",
    "print(e.grad)\n",
    "print(y_pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-HTDCpBCZuH"
   },
   "source": [
    "# Exercícios (enunciados melhorados em 24/aug/2020)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsOThnt8fDJV"
   },
   "source": [
    "1. Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "62nZAfUoCZu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad= 3\n"
     ]
    }
   ],
   "source": [
    "def J_func(w, x, y):\n",
    "    # programe a função J_func, para facilitar\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Calcule o gradiente usando a regra diferenças finitas\n",
    "# Confira com o valor já calculado anteriormente\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1)\n",
    "grad = 3\n",
    "print('grad=', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_Sx1QXZxJ3u"
   },
   "source": [
    "2. Minimizando $J$ pelo gradiente descendente\n",
    "\n",
    "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
    "\n",
    "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar as funções `J`e `dJ` criadas no exercicio 1.\n",
    "\n",
    "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNszCOED1Wtu"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "iteracoes = 20\n",
    "\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1)\n",
    "\n",
    "for i in range(iteracoes):\n",
    "    print('i =', i)\n",
    "    J = J_func()\n",
    "    print('J=', J)\n",
    "    grad = \n",
    "    print('grad =',grad)\n",
    "    w = \n",
    "    print('w =', w)\n",
    "\n",
    "# Plote o gráfico da loss J pela iteração i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBXxBmWGK3IU"
   },
   "source": [
    "3. Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMP4d5vtHtqy"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "iteracoes = 20\n",
    "\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1, requires_grad=True)\n",
    "\n",
    "for i in range(iteracoes):\n",
    "    print('i =', i)\n",
    "    J = J_func()\n",
    "    print('J=', J)\n",
    "    grad =    \n",
    "    print('grad =',grad)\n",
    "    w = \n",
    "    print('w =', w)\n",
    "\n",
    "# Plote aqui a loss pela iteração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GulfYtzBMx2e"
   },
   "source": [
    "4. Compare os resultados dos exercícios 2 e 3. É esperado que sejam iguais. \n",
    "\n",
    "a) Existe alguma restrição nos valores de $\\Delta w$ na função `dJ_diff` do gradiente por diferenças finitas?\n",
    "\n",
    "b) Quais são as vantagens de calcular o gradiente usando `backward()` pelo grafo computacional (autograd)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35I5w8EZdjIo"
   },
   "source": [
    "5. Qual o custo (entropia cruzada) esperado por exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
    "\n",
    "A equação da entropia cruzada é:\n",
    "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
    "Onde:\n",
    "\n",
    "- K é o número de classes;\n",
    "\n",
    "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, y é um vetor one-hot;\n",
    "\n",
    "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
    "\n",
    "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
    "\n",
    "- K = número de classes\n",
    "\n",
    "- B = batch size\n",
    "\n",
    "- D = dimensão de qualquer vetor do modelo\n",
    "\n",
    "- LR = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UNdHqgSB6S9"
   },
   "source": [
    "## Fim do notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercicios de Autograd do PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
